{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experimentation_baseline_climate_sentiment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0hlFRNAvVQi/xfUhW12JE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GusMalija/Master-Thesis-Project-Augustine-Malija/blob/main/experimentation_baseline_climate_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oen0lLTJjEvp",
        "outputId": "76bb16ef-ff1a-4fd2-b234-71e83a930479"
      },
      "source": [
        "#calling important libraries\n",
        "import pandas as pd #for data manipulation\n",
        "import numpy as np #for data manipulation\n",
        "import matplotlib as plt #for plotting\n",
        "import os #for ease of python system interaction\n",
        "import sys\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSfRHiZinjcz"
      },
      "source": [
        "#retrieving the data\n",
        "url = \"https://raw.githubusercontent.com/GusMalija/Master-Thesis-Project-Augustine-Malija/main/Data/twitter_sentiment_data.csv\"\n",
        "\n",
        "labeled_data = pd.read_csv(url)\n",
        "#checking the dataset features\n",
        "labeled_data.keys()\n",
        "labeled_data.head()\n",
        "\n",
        "#randomly selecting 2000 data points\n",
        "labeled_data = labeled_data.sample(n = 20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI1eEvQsUBjY"
      },
      "source": [
        "#extracting only tweets as features\n",
        "features = labeled_data.iloc[:,1].values\n",
        "#extracting labels\n",
        "stance_labels = labeled_data.iloc[:,0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdI4T0vGWqNE"
      },
      "source": [
        "#preprocessing tweets\n",
        "processed_features = []\n",
        "\n",
        "for sentence in range(0, len(features)):\n",
        "    #Removing special characters\n",
        "    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "    #removing single characters\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "    #Removing single characters from the start\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "    #Substituting multiple spaces with single space\n",
        "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "    #Removing prefixed 'b'\n",
        "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "    #removing @\n",
        "    processed_feature = re.sub(r'@[\\w]+','',processed_feature)\n",
        "    #removing numbers\n",
        "    processed_feature = re.sub(r'[0-9]+', '', processed_feature)\n",
        "    # Removing hashtags\n",
        "    processed_feature = re.sub(r'#\\w*', ' ', processed_feature)\n",
        "    #removing url\n",
        "    processed_feature = re.sub(r\"http\\S+\", \"\", processed_feature)\n",
        "    #Converting to Lowercase\n",
        "    processed_feature = processed_feature.lower()\n",
        "\n",
        "    processed_features.append(processed_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQrTdiszTGy_"
      },
      "source": [
        "#specifying parameters\n",
        "parameters = [\n",
        "    {\n",
        "    'vect__max_df': (0.9,), #ignore terms with frequency higher than aforementioned\n",
        "        'vect__min_df': (2,), #ignore lower frequencies than aforementioned\n",
        "        'vect__ngram_range': ((1, 2),), #unigrams and bigrams\n",
        "        'clf__estimator__kernel': ['rbf'], #gausian kernel\n",
        "    'clf__estimator__gamma': [1e0,], #a gamma of zero\n",
        "        'clf__estimator__C': [1,],\n",
        "        'clf__estimator__class_weight': [None, \"balanced\"] #balanced weight\n",
        "    } ,\n",
        "    {\n",
        "        'vect__max_df': (0.9,),\n",
        "        'vect__min_df': (2,),\n",
        "        'vect__ngram_range': ((1, 2),),\n",
        "        'clf__estimator__kernel': ['linear'], #linear kernel\n",
        "    'clf__estimator__C': [1,]\n",
        "    }\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqXP2bg06aDN"
      },
      "source": [
        "# building a pipeline\n",
        "pipeline = Pipeline([('vect', TfidfVectorizer()),   #max_features = 2500, stop_words=stopwords.words(\"english\")                  \n",
        "    ('clf', OneVsRestClassifier(SVC(probability=True))),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gfM_jGEQ3Oa"
      },
      "source": [
        "#splitting the dataset to trian and test set\n",
        "#80 percent of data for training, 20 percent for testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_features, stance_labels, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmRPNvoj7LEP"
      },
      "source": [
        "#grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid=parameters, n_jobs=-1, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0EPZZmd7Puz",
        "outputId": "729164f0-e54d-435e-83bd-d6e7aea3370c"
      },
      "source": [
        "#fitting the classifier\n",
        "classifier = grid_search.fit(X_train, y_train)\n",
        "\n",
        "#predicting\n",
        "y_predict = classifier.predict(X_test)\n",
        "\n",
        "#2000, accuracy 64\n",
        "#5000, accuracy 65\n",
        "#6000, acuracy 67\n",
        "#10000, accuracy 70\n",
        "#15000, accuracy 72\n",
        "#20000, accuracy 73"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 181.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4D5ONIlB8W7Y",
        "outputId": "602f24c7-45dd-48f1-c14e-c0aae8e5f0b9"
      },
      "source": [
        "#confusion matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, y_predict))\n",
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 136   84  122   26]\n",
            " [  24  318  276   63]\n",
            " [  22  143 1788  169]\n",
            " [   6   15  149  659]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.72      0.37      0.49       368\n",
            "           0       0.57      0.47      0.51       681\n",
            "           1       0.77      0.84      0.80      2122\n",
            "           2       0.72      0.79      0.75       829\n",
            "\n",
            "    accuracy                           0.73      4000\n",
            "   macro avg       0.69      0.62      0.64      4000\n",
            "weighted avg       0.72      0.73      0.71      4000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}